<!DOCTYPE html>
<!--
    Plain-Academic by Vasilios Mavroudis
    Released under the Simplified BSD License/FreeBSD (2-clause) License.
    https://github.com/mavroudisv/plain-academic
-->

<html lang="en">
<head>
  <title>Fanjun (Frank) Bu</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.0/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>
  <link href='https://fonts.googleapis.com/css?family=Oswald:700' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Jost' rel='stylesheet'>
  <style>
  body {
      font-family: 'Jost';font-size: 16px;
  }
  </style>
</head>
<body>


<!-- Navigation -->
	<nav class="navbar navbar-inverse navbar-static-top" role="navigation">
	  <div class="container">
		<div class="navbar-header">
		  <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
						<span class="sr-only">Toggle navigation</span>
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
					</button>
		</div>

		<!-- Collect the nav links, forms, and other content for toggling -->
		<div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
		  <ul class="nav navbar-nav">
				  <li><a href="index.html">Home</a></li>
				  <li><a href="./CV.pdf">CV</a></li> 
		  </ul>
		</div>
	  </div>
	</nav>
  
  <!-- Page Content -->
    <div class="container">

        <div class="row">

            <!-- Entries Column -->
            <div class="col-md-8">
                
                <!-- Main Image -->
                <img class="img-responsive " src="self.jpg" width="200" alt=""><br>
                
                <div style="margin-top:3%; text-align:justify;">        
                  I am a Ph.D. student at Cornell Tech under the supervision of <a href="https://tech.cornell.edu/people/wendy-ju/">Prof. Wendy Ju</a>. 
                  My research focuses on understanding how people interact with emerging technologies. 
                  In particular, I am interested in human-robot interaction and the challenges it presents. 
                  My approach involves utilizing the Wizard-of-Oz technique for in-the-wild deployments to simulate robots' autonomy 
                  and elicit natural interaction behaviors. By leveraging unique interaction data collected through the Wizard-of-Oz technique, 
                  I aim to bootstrap robots' social intelligence. Through my research, I aspire to provide valuable guidelines for the design of future technologies that seamlessly integrate 
                  into human environments.
                </div>
            </div> 

            <!-- Contact Info on the Sidebar -->
            <div class="col-md-4">
                <div style="font-family: 'Oswald', sans-serif; font-size: 32px;"><b>Fanjun (Frank) Bu</b></div><br>
               Ph.D. Student
                <p>Computer Science Department<br>
                Cornell University, Cornell Tech<br>
                </p>
            </div>
            
            
            <!-- Links on the Sidebar -->
            <div class="col-md-4" style="margin-top:2%">
              <dd><a href="https://scholar.google.com/citations?user=Th-dmNUAAAAJ&hl=en">Google Scholar</a></dd> 
              <dd><a href="https://github.com/FrankBu0616">Github</a></dd> 
            </div>
            
            
            <!-- Publications -->
            <div class="col-md-8">    
                <h3 id="publications">Publications</h3>

                <ol type="1">
                  <li><strong>Fanjun Bu</strong>, Stacey Li, David Goedicke, Mark Colley, Gyanendra Sharma, Wendy Ju.
                    “Portobello: Extending Driving Simulation from the Lab to the Road”. In:
                    Proceedings of the CHI Conference on Human Factors in Computing Systems. 2024,
                    pp. 1–13.</li>
                  <a href="./Portobello_draft.pdf">[Preprint]</a> <a href="https://youtu.be/dRTAbvt4P9A"> [Video Demo]</a>
                  <li>Barry Brown, <strong>Fanjun Bu</strong>, Ilan Mandel, Wendy Ju. “Trash in Motion: Emergent
                    interactions with robotic trashcans in a public square”. In: Proceedings of the CHI
                    Conference on Human Factors in Computing Systems. 2024, pp. 1–15.</li>
                  <a href="./Trash_in_Motion_draft.pdf">[Preprint]</a>
                  <li><strong>Fanjun Bu*</strong>, Ilan Mandel*, Wen-Ying Lee, Wendy Ju. “Trash Barrel Robots in the
                      City”. In: Companion of the 2023 ACM/IEEE International Conference on
                      Human-Robot Interaction. HRI ’23. Stockholm, Sweden: Association for
                      Computing Machinery, 2023, pp. 875–877.</li>
                  <a href="https://dl.acm.org/doi/abs/10.1145/3568294.3580206">[Paper]</a><a href="https://www.youtube.com/watch?v=pS5ptE2USSo&t=1s&ab_channel=FanjunBu">[Video]</a>
                  <li>David Goedicke, Alexandra WD Bremers, Sam Lee, <strong>Fanjun Bu</strong>, Hiroshi Yasuda,
                    Wendy Ju. “XR-OOM: MiXed Reality driving simulation with real cars for research
                    and design”. In: Proceedings of the 2022 CHI Conference on Human Factors in
                    Computing Systems. 2022, pp. 1–13.</li>
                  <a href="https://dl.acm.org/doi/abs/10.1145/3491102.3517704">[Paper]</a>
                  <li>Jan Ondras, Abrar Anwar, Tong Wu, <strong>Fanjun Bu</strong>, Malte Jung, Jorge Jose Ortiz,
                    Tapomayukh Bhattacharjee. “Human-robot commensality: Bite timing prediction
                    for robot-assisted feeding in groups”. In: 6th Annual Conference on Robot Learning.
                    2022.</li>
                  <a href="https://openreview.net/forum?id=7ZcePvChS7u">[Paper]</a>
                  <li><strong>Fanjun Bu</strong>, Chien-Ming Huang. “Object permanence through audio-visual
                    representations”. In: IEEE Access 9 (2021), pp. 131574–131582.</li>
                  <a href="https://ieeexplore.ieee.org/document/9547333">[Paper]</a> <a href="https://archive.data.jhu.edu/dataset.xhtml?persistentId=doi:10.7281/T1/EP0W7Y">[Dataset]</a>
                      <a href="https://youtu.be/Rj-ZZf3r4g8">[Video]</a>
                  <li>Konstantinos Chatzilygeroudis, Bernardo Fichera, Ilaria Lauzana, <strong>Fanjun Bu</strong>,
                    Kunpeng Yao, Farshad Khadivar, Aude Billard. “Benchmark for bimanual robotic
                    manipulation of semi-deformable objects”. In: IEEE Robotics and Automation
                    Letters 5.2 (2020), pp. 2443–2450.</li>
                      <a href="https://ieeexplore.ieee.org/abstract/document/8989777">[Paper]</a>
                </ol> 
            </div>

            <div class="col-md-8">    
              <h3 id="publications">Teaching Assistantships</h3>

              <ol>
                <li> [Spring 2023] INFO5755/INFO6755/CS5755/CS675 <a href="https://github.com/FAR-Lab/Mobile_HRI_Lab_Hub">Mobile Human Robot Interaction Design</a> </li>
                <strong>TA Award</strong>
                <li> [Fall 2021] CS4750/CS5750/ECE4770/MAE4760.  <a href="https://www.cs.cornell.edu/courses/cs5750/2021fa/"> Foundations of Robotics</a> </li>
                <strong>TA Award</strong>
              </ol> 
          </div>

            <div class="col-md-8">    
              <h3 id="Press">Press</h3>

              <ol type="1">
                <li> Staff, <a href="https://www.cnn.com/videos/business/2023/04/11/robotic-trash-cans-nyc-cornell-contd-orig-fj.cnn-business">"These robotic trash cans were filmed to test human-robotic interactions.</a> Watch what happened," CNN Business, April 12, 2023.</li>
                <li> Ayesha Rascoe, <a href="https://www.npr.org/2023/04/16/1170293941/researchers-released-robot-trash-cans-in-nyc-to-see-how-people-would-react#:~:text=Researchers%20released%20robot%20trash%20cans,how%20people%20would%20react%20%3A%20NPR&text=Books%20Podcast-,Researchers%20released%20robot%20trash%20cans%20in%20NYC%20to%20see%20how,interacted%20with%20the%20robo%2Dbins.">"Researchers released robot trash cans in NYC to see how people would react,"</a> National Public Radio (NPR), April 16, 2023. </li>
                <li> Patricia Waldron, <a href="https://news.cornell.edu/stories/2023/04/almost-everyone-likes-helpful-trash-robot">"(Almost) everyone likes a helpful trash robot,"</a> Cornell Chronicle, April 19, 2023.</li>
                <li> Mike Snider, <a href="https://www.usatoday.com/story/news/nation/2023/04/15/video-robot-trash-cans-new-york-city/11666593002/">"Robots in the Big Apple: Robo-trash cans patrolling New York plaza make friends, creep out some,"</a> USA TODAY, April 15, 2023. </li>
                <li> Evan Ackerman, <a href="https://spectrum.ieee.org/nyc-trash-robots">"Humans (Mostly) Love Trash Robots > Simple robots wander NYC asking for trash and recycling, and it’s adorable,"</a> IEEE Spectrum, Mar 10, 2023.</li> 
              </ol> 
          </div>            
        </div>
    </div>
    <!-- /.container -->
    
    <!-- Other people may like it too! -->
    <a style="color:#b5bec9;font-size:0.8em; float:right;" href="https://github.com/mavroudisv/plain-academic">Plain Academic</a> 
    
</body>

</html>
